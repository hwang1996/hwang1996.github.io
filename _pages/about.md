---
permalink: /
title: "About Me"
excerpt: "About Me"
author_profile: true
redirect_from: 
  - about/
  - about.html
---

<!-- I am a tenure-track Assistant Professor of the AI Thrust at [The Hong Kong University of Science and Technology (Guangzhou)](https://hkust-gz.edu.cn/). I received my Ph.D. at School of Computer Science and Engineering, Nanyang Technological University, supervised by [Prof. Miao Chun Yan](https://dr.ntu.edu.sg/cris/rp/rp00084). My co-supervisor is [Prof. Guosheng Lin](https://guosheng.github.io). I also work closely with [Prof. Steven Hoi](https://sites.google.com/view/stevenhoi/home). I was an intern working with [Jiashi Feng](https://sites.google.com/site/jshfeng/home) at TikTok, Singapore. 

My general research interests lie in the development of AI-powered perception and generation algorithms for multimodal data, including text, images, videos, and 3D shapes. Recently, we are working on projects of <font color=RoyalBlue>3D reconstruction</font> and <font color=RoyalBlue>LLM-based agents</font>. Please drop me an email if you are interested in collaborations. -->


I am a tenure-track Assistant Professor of AI at [The Hong Kong University of Science and Technology (Guangzhou)](https://hkust-gz.edu.cn/). I obtained my PhD from [Nanyang Technological University](https://www.ntu.edu.sg/), Singapore; I was a researcher with TikTok and Horizon Robotics. 

My research interests include spatial intelligence, 3D Gaussian Splatting, LLM agents, and related areas. I have published over 50 papers in top-tier conferences and journals, including TPAMI, IJCV, CVPR, and NeurIPS. I also serve as an area chair and reviewer for multiple leading conferences. I received the rising star award on ICCSE 2025, Guangdong provincial talent award (广东省高层次青年人才) and etc. 

<!-- <font color=RoyalBlue>I am looking for self-motivated PhD students, RAs and interns.</font>
Please check my [recruitment page](https://wanghao.tech/recruitment/). -->


<!-- <br /> -->


<!-- News
======

* [Oct 2023] Our paper on LLM agents in Avalon gameplay is released!
* [Feb 2023] Our paper is accepted to CVPR 2023.
* [Jun 2022] Our paper is accepted to TIP.
* [Jun 2022] Our paper is accepted to ACM MM 2022.
* [May 2022] Our paper is accepted to TPAMI. -->


<!-- <br /> -->

<!-- Highlighted projects -->
<!-- Highlighted Projects
====== -->


## Highlighted Projects

<!-- 增加一个 style 块，统一调整字体大小和响应式 -->
<style>
.projects-grid .project-card { width:300px; border:1px solid #eee; padding:0.5rem; box-sizing:border-box; }
.projects-grid .project-card h4 { margin:0.5rem 0; font-size:18px; line-height:1.2; }
.projects-grid .project-card p { margin:0; font-size:14px; color:#555; }
.projects-grid .project-card img { width:100%; height:auto; display:block; }
@media (max-width:600px){
  .projects-grid { flex-direction:column; }
  .projects-grid .project-card { width:100%; }
  .projects-grid .project-card h4 { font-size:16px; }
  .projects-grid .project-card p { font-size:13px; }
}
</style>

<div class="projects-grid" style="display:flex;flex-wrap:wrap;gap:1rem;">
  <div class="project-card">
    <a href="https://3dagentworld.github.io/vggt4d/" target="_blank" rel="noopener">
      <img src="/images/projects/vggt4d.png" alt="VGGT4D" />
    </a>
    <h4>
      <a href="https://3dagentworld.github.io/vggt4d/" target="_blank" rel="noopener">VGGT4D</a>
    </h4>
    <!-- <p><a href="https://arxiv.org/abs/2502.15633" target="_blank" rel="noopener">[Paper]</a></p> -->
  </div>

  <div class="project-card">
    <a href="https://3dagentworld.github.io/opengs-slam/" target="_blank" rel="noopener">
      <img src="/images/projects/opengs-slam.png" alt="OpenGS-SLAM" />
    </a>
    <h4>
      <a href="https://3dagentworld.github.io/opengs-slam/" target="_blank" rel="noopener">OpenGS-SLAM</a>
    </h4>
    <!-- <p><a href="https://arxiv.org/abs/2502.15633" target="_blank" rel="noopener">[Paper]</a></p> -->
  </div>
</div>


<!-- <br />
<br /> -->


<!-- Selected Publications
====== -->
## Selected Publications

<ul>

<li>
  MultiGO: Towards Multi-level Geometry Learning for Monocular 3D Textured Human Reconstruction 
  <br/> 
  Gangjian Zhang, Nanjie Yao, Shunsi Zhang, Hanfeng Zhao, Guoliang Pang, Jian Shu, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>IEEE Conference on Computer Vision and Pattern Recognition</i>
    <strong>(CVPR 2025)</strong> <br>
</li>

<li>
  RGB-Only Gaussian Splatting SLAM for Unbounded Outdoor Scenes 
  <br/> 
  Sicheng Yu, Chong Cheng, Yifan Zhou, Xiaojun Yang, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>International Conference on Robotics and Automation</i>
    <strong>(ICRA 2025)</strong> <br>
</li>

<li>
  SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation 
  <br/> 
  Junlong Ren, Hao Wu, Hui Xiong, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>International Conference on Robotics and Automation</i>
    <strong>(ICRA 2025)</strong> <br>
</li>

<li>
  Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting <a href="https://openreview.net/forum?id=56vHbnk35S">[Paper]</a>
  <br/> 
  Chong Cheng, Gaochao Song, Yiyang Yao, Gangjian Zhang, Qinzheng Zhou, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>International Conference on Learning Representations</i>
    <strong>(ICLR 2025)</strong> <br>
</li>


<li>
  GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface Reconstruction in Open Scenes <a href="https://arxiv.org/pdf/2411.01853">[Paper]</a>
  <br/> 
  Gaochao Song, Cheng Chong, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>Conference on Neural Information Processing Systems</i>
    <strong>(NeurIPS 2024)</strong> <br>
</li>

<li>
  LLM-Based Agent Society Investigation: Collaboration and Confrontation in Avalon Gameplay <a href="https://arxiv.org/abs/2310.14985">[Paper]</a> <a href="https://github.com/3DAgentWorld/LLM-Game-Agent">[Code]</a>
  <br/> 
  Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, <strong><font color="black">Hao Wang*</font></strong>. (* denotes corresponding author)
  <br/><i>The 2024 Conference on Empirical Methods in Natural Language Processing</i>
    <strong>(EMNLP 2024 Main)</strong> <br>
</li>

<li>
  HMR-Adapter: A Lightweight Adapter with Dual-Path Cross Augmentation for Expressive Human Mesh Recovery <a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681641">[Paper]</a>
  <br/> 
  Wenhao Shen, Wanqi Yin, <strong><font color="black">Hao Wang*</font></strong>, Chen Wei, Zhongang Cai, Lei Yang, Guosheng Lin*. (* denotes corresponding author)
  <br/><i>ACM International Conference on Multimedia</i>
    <strong>(ACM MM-2024)</strong><br>
</li>

<li>
  Learning Temporal Variations for 4D Point Cloud Segmentation <a href="https://arxiv.org/abs/2207.04673">[Paper]</a>
  <br/> 
  Hanyu Shi, Jiacheng Wei, <strong><font color="black">Hao Wang</font></strong>, Fayao Liu, Guosheng Lin.
  <br/><i>International Journal of Computer Vision</i>
    <strong>(IJCV-2024)</strong> [<alert>IF:19.5</alert>]<br>
</li>

<li>
  ManiCLIP: Multi-Attribute Face Manipulation from Text <a href="https://link.springer.com/article/10.1007/s11263-024-02088-6">[Paper]</a> <a href="https://github.com/hwang1996/ManiCLIP">[Code]</a>
  <br/> 
  <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Ana García del Molino, Anran Wang, Jiashi Feng, Zhiqi Shen.
  <br/><i>International Journal of Computer Vision</i>
    <strong>(IJCV-2024)</strong> [<alert>IF:19.5</alert>]<br>
</li>

<li>
    COM3D: Leveraging Cross-View Correspondence and Cross-Modal Mining for 3D Retrieval <a href="https://arxiv.org/abs/2405.04103">[Paper]</a>
    <br/>
    Hao Wu, Ruochong LI, <strong><font color="black">Hao Wang*</font></strong>, Hui Xiong. (* denotes corresponding author)
    <br/>
    <i>IEEE Conference on Multimedia Expo </i> <strong>(ICME-2024 Oral)</strong><br>
</li>
<li>
    TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision <a href="https://arxiv.org/abs/2303.13273">[Paper]</a> <a href="https://github.com/plusmultiply/taps3d">[Code]</a>
    <br/>
    Jiacheng Wei*, <strong><font color="black">Hao Wang*</font></strong>, Jiashi Feng, Guosheng Lin, Kim-Hui Yap. (* denotes equal contributions)
    <br/>
    <i>IEEE Conference on Computer Vision and Pattern Recognition </i> <strong>(CVPR-2023)</strong><br>
</li>
<li>
    Cross-Modal Graph with Meta Concepts for Video Captioning <a href="https://arxiv.org/abs/2108.06458">[Paper]</a> <a href="https://github.com/hwang1996/Meta-Concepts-for-Video-Captioning">[Code]</a>
    <br/>
    <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Steven Hoi, Chunyan Miao.
    <br/><i>IEEE Transactions on Image Processing</i>
    <strong>(TIP-2022)</strong> [<alert>IF:11.041</alert>]<br>
</li>
<li>
    Paired Cross-Modal Data Augmentation for Fine-Grained Image-to-Text Retrieval <a href="https://arxiv.org/abs/2207.14428">[Paper]</a>
    <br/>
    <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Steven Hoi, Chunyan Miao.
    <br/><i>ACM International Conference on Multimedia</i>
    <strong>(ACM MM-2022)</strong><br>
</li>
<li>
    Learning Structural Representations for Recipe Generation and Food Retrieval <a href="https://arxiv.org/abs/2110.01209">[Paper]</a>
    <br/>
    <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Steven Hoi, Chunyan Miao.
    <br/><i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>
    <strong>(TPAMI-2022)</strong> [<alert>IF:24.314</alert>]<br>
</li>
<li>
    Cycle-Consistent Inverse GAN for Text-to-Image Synthesis <a href="https://arxiv.org/abs/2108.01361">[Paper]</a>
    <br/>
    <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Steven Hoi, Chunyan Miao.
    <br/><i>ACM International Conference on Multimedia</i>
    <strong>(ACM MM-2021)</strong><br>
</li>
<li>
    Structure-Aware Generation Network for Recipe Generation from Images <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/5757_ECCV_2020_paper.php">[Paper]</a> <a href="https://github.com/hwang1996/SGN">[Code]</a>
    <br/> 
    <strong><font color="black">Hao Wang</font></strong>, Guosheng Lin, Steven Hoi, Chunyan Miao.
    <br/><i>European Conference on Computer Vision</i>
    <strong>(ECCV-2020)</strong> <br>
</li>
<li>
    SpSequenceNet: Semantic Segmentation Network on 4D Point Clouds <a href="http://openaccess.thecvf.com/content_CVPR_2020/html/Shi_SpSequenceNet_Semantic_Segmentation_Network_on_4D_Point_Clouds_CVPR_2020_paper.html">[Paper]</a> <a href="https://github.com/dante0shy/SpSequenceNet">[Code]</a>
    <br/> 
    Hanyu Shi, Guosheng Lin, <strong><font color="black">Hao Wang</font></strong>, Tzu-Yi Hung, Zhenhua Wang.
    <br/><i>IEEE Conference on Computer Vision and Pattern Recognition</i>
    <strong>(CVPR-2020)</strong> <br>
</li>
<li>
    FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging <a href="https://arxiv.org/abs/1909.11946">[Paper]</a>
    <br/> 
    Doyen Sahoo, <strong><font color="black">Hao Wang</font></strong>, Shu Ke, Xiongwei Wu, Hung Le, Palakorn Achananuparp, Ee-Peng Lim, Steven Hoi.
    <br/><i> ACM SIGKDD conference, 2019</i>
    <strong>(KDD-2019)</strong> <br>
</li>
<li>
    Learning Cross-Modal Embeddings with Adversarial Networks for Cooking Recipes and Food Images <a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Learning_Cross-Modal_Embeddings_With_Adversarial_Networks_for_Cooking_Recipes_and_CVPR_2019_paper.html">[Paper]</a> <a href="https://github.com/hwang1996/ACME">[Code]</a>
    <br/> 
    <strong><font color="black">Hao Wang</font></strong>, Doyen Sahoo, Chenghao Liu, Ee-peng Lim, Steven C. H. Hoi.
    <br/><i>IEEE Conference on Computer Vision and Pattern Recognition</i>
    <strong>(CVPR-2019)</strong> <br>
</li>
</ul>